{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736651df-ed87-49db-aa31-864363fab0fb",
   "metadata": {},
   "source": [
    "# Intermediate: Classifying stellar flares with *stella*\n",
    "\n",
    "This notebook tutorial demonstrates how to determine whether or not a TESS light curve contains a stellar flare using the *stella* package. AS: I've commented out code that I may use later but doesn't strictly contribute to the current flow!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451e9e5-7904-4cd6-b7a6-4c77ffde8896",
   "metadata": {},
   "source": [
    "# Learning objectives\n",
    "- Understand Basic structure of neural networks and how they can be applied to astronomy\n",
    "- Be able to determine the quality of a neural network's training\n",
    "- Understand how stellar flares can be identified in light curves "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d554294-4262-4589-be33-db8466f397e0",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a525e54-ed94-4287-bc32-3ad12d67cf41",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672bc822-65ad-4bf0-984a-85b22d6aa246",
   "metadata": {},
   "source": [
    "Stellar flares touch disparate corners of astrophysics. From the perspective of studying exoplanets, they may control atmospheric evolution and erosion; from the perspective of studying stars, they provide insight into stellar interiors and magnetic fields (cite a bunch)  \n",
    "\n",
    "The <a href=https://archive.stsci.edu/missions-and-data/tess>TESS mission</a>, with its relatively long-duration observations (28 days at a time) at a relatively short cadence (as low as 20 seconds), is well suited for searches of time-dependent stellar effects such as flares. A number of studies have used the observatory to detect flares (e.g., <a href=https://iopscience.iop.org/article/10.3847/1538-3881/abac0a/meta>Feinstein et al. 2020</a>, <a href=https://iopscience.iop.org/article/10.3847/1538-3881/ab5d3a/meta>Günther et al. 2020</a>)  \n",
    "\n",
    "This notebook will explore stellar flare classification with neural networks. We will hew closely to the <a href=https://spacetelescope.github.io/hellouniverse/intro.html>Hello Universe example</a>, which makes use of the Stella code developed in <a href=https://iopscience.iop.org/article/10.3847/1538-3881/abac0a/meta>Feinstein et al. 2020</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19660d8-7cda-474f-8953-75505d467803",
   "metadata": {},
   "source": [
    "# Part 1: Imports and installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab6b35-2dfe-4053-824e-17c75085eb25",
   "metadata": {},
   "source": [
    "# Import Statements\n",
    "\n",
    "* `numpy` is used for array manipulation.\n",
    "* `tarfile` is used for unpacking datasets.\n",
    "\n",
    "* `astropy.io.fits` allows us to interact with the FITS files.\n",
    "* `astropy.utils.data.download_data` is used to download the data at the location specified by a URL.\n",
    "* `matplotlib.pyplot` is used to display images and plot datasets.\n",
    "* `stella` is used to manipulate CNNs for classifying stellar flares.\n",
    "* `keras` is used to manuipulate CNN architecture.\n",
    "* `sklearn` is a machine learning library with helpful utility functions for deep learning.\n",
    "* `lightkurve` allows us to easily interact with TESS light curves.\n",
    "* `os` to access the local operating system.\n",
    "* `random` to access the Python random number generator.\n",
    "* `TensorFlow` to manipulate low-level machine learning functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1b24e-8c20-4a95-84c6-a977667052f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# arrays\n",
    "import numpy as np\n",
    "\n",
    "# unpacking files \n",
    "import tarfile\n",
    "\n",
    "# fits\n",
    "from astropy.io import fits\n",
    "from astropy.utils.data import download_file\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "# sklearn for performance metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# lightkurve\n",
    "from lightkurve.search import search_lightcurve\n",
    "\n",
    "# python built-ins\n",
    "import os\n",
    "import random\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7d9e9-ff44-4cb6-a396-53d47dc09cd6",
   "metadata": {},
   "source": [
    "In a previous version of TIKE, the `stella` package was not available. This required using the a `conda` command to install the package directly from its [GitHub repository](https://github.com/afeinstein20/stella).\n",
    "\n",
    "To keep code compatible across versions, we'll wrap this import in a try/except block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8439c9-ea50-4faa-9ee7-920799199e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Try to import stella\n",
    "try:\n",
    "    import stella\n",
    "    \n",
    "# If import fails, install and then import\n",
    "except:\n",
    "    conda run pip install git+https://github.com/afeinstein20/stella\n",
    "    import stella"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b4c57-1b74-4d50-acc1-fdde72993fac",
   "metadata": {},
   "source": [
    "# Part 2: Science"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e379a8-a79b-4058-b01c-0916e11c54e1",
   "metadata": {},
   "source": [
    "## Downloading the *stella* data\n",
    "Classification algorithms such as neural networks require large datasets to \"learn\" what they're looking for. Curating such a dataset is often a time consuming, manual process. Luckily for us, the authors of the `stella` package have curated a set of TESS light curves that do and do not exhibit flares. We'll use this dataset to train our neural network.\n",
    "\n",
    "This labeled dataset is hosted by MAST, as part of the [Hello Universe](https://archive.stsci.edu/hello-universe) machine learning project. Let's load in the link to that file now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0576a44e-b7f0-40cb-9113-6b7227634db3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_url = 'https://archive.stsci.edu/hlsps/hellouniverse/hellouniverse_stella_500.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36df2b16-6822-41fa-b527-dc1eb781a131",
   "metadata": {},
   "source": [
    "Next, we download and open the file. setting ``cache=True`` means that the file won't be downloaded again if it's already been downloaded before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d72e552-4102-4395-90e6-23aafa735657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file = tarfile.open(download_file(file_url, cache=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6594a609-b0d3-4aaa-a05c-aa9cb7f7c8da",
   "metadata": {},
   "source": [
    "Next, we extract the contents of the file, placing the folder within in the current directory — the '.' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d8b7f-8397-489c-9e6a-a9bd21696ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file.extractall('.')\n",
    "file.close() # be sure to close files when you're finished with them!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686fe004-3edf-4c79-b79e-cad8cb605b52",
   "metadata": {},
   "source": [
    "At this point, the data is downloaded and accessible to our server instance. Let's use ``stella`` to construct a ``FlareDataSet`` object with the data that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2385bd3-5f27-433f-ae32-ded4f5df57c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = './hellouniverse_stella_500/'\n",
    "ds = stella.FlareDataSet(fn_dir=data_dir,\n",
    "                         catalog=data_dir+'Guenther_2020_flare_catalog.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841da48-ee05-44fe-a688-ccb8bd3b619e",
   "metadata": {},
   "source": [
    "What is this `ds` variable, and what did the function spit out? \n",
    "\n",
    "`ds` is a `FlareDataSet`; a reformatted collection of training datasets for neural networks, with the data arrays shaped for neural network inputs and labeled by whether or not they contain a flare. These labels are provided by the `catalog`, which we specify in the `FlareDataSet` initialization.\n",
    "\n",
    "The text printed to the notebook first lets us know that the files are being read in (as intended) then provides a progress bar (courtesy of the `tqdm` package). Then, we're given the statistics of the dataset: that 502 light curves contain flares, and 1342 do not. The mistmatch between lightcurves with and without flares is known as a \"class imbalance\". We'll explore in an exercise how the class imbalance impacts the neural network performance.\n",
    "\n",
    "Now, let's clean up this dataset a little bit and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245b3c8b-2d44-486a-bf1f-bfe77741df6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove lightcurves with NaNs from training, test and validation data\n",
    "def remove_nans(input_data):\n",
    "    \"\"\"\n",
    "    Determine indices of files without NaNs.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "        :input_data: data that will be passed to CNN (array-like)\n",
    "        \n",
    "    Outputs\n",
    "    -------\n",
    "        :idx: indices without NaNs. (list)\n",
    "    \"\"\"\n",
    "\n",
    "    idx = []\n",
    "    for k in range(np.shape(input_data)[0]):\n",
    "        if len(input_data[k, :, :][np.isnan(input_data[k, :, :])]) == 0:\n",
    "            idx.append(k)\n",
    "    return idx\n",
    "\n",
    "\n",
    "# find indices in train, test and validation sets without NaNs\n",
    "idx_train = remove_nans(ds.train_data)\n",
    "idx_test = remove_nans(ds.test_data)\n",
    "idx_val = remove_nans(ds.val_data)\n",
    "\n",
    "ds.train_data = ds.train_data[idx_train]\n",
    "ds.train_labels = ds.train_labels[idx_train]\n",
    "\n",
    "ds.test_data = ds.test_data[idx_test]\n",
    "ds.test_labels = ds.test_labels[idx_test]\n",
    "\n",
    "ds.val_data = ds.val_data[idx_val]\n",
    "ds.val_labels = ds.val_labels[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b53170-443c-4be1-aec3-e673ad051680",
   "metadata": {},
   "source": [
    "Now, let's plot a subset of the data. Let's choose random ones. Try executing the next cell a few times in a row — you'll get different plots each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ca817-092f-4c33-b471-6d3bf7d51534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select random image indices:\n",
    "example_ids = np.random.choice(len(ds.train_labels), 16)\n",
    "\n",
    "# pull the lightcurves and labels for these selections\n",
    "example_lightcurves = [ds.train_data[j] for j in example_ids]\n",
    "example_labels = [ds.train_labels[j] for j in example_ids]\n",
    "\n",
    "\n",
    "# initialize your figure\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "\n",
    "# create two dictionaries with the same keys mapping to corresponding colors and flare property\n",
    "colors = {1: 'r', 0: 'k'}\n",
    "titles = {1: 'Flare', 0: 'Non-flare'}\n",
    "\n",
    "# loop through the randomly selected images and plot with labels. The red ones indeed have flares!\n",
    "for i in range(len(example_ids)):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.plot(example_lightcurves[i], color=colors[example_labels[i]])\n",
    "    plt.title(titles[example_labels[i]])\n",
    "    plt.xlabel('Cadences')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c27885-2da9-435e-a9f0-f173bd77e36c",
   "metadata": {},
   "source": [
    "It seems like we have our dataset, and we're flagging things that look like flares. Awesome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713cd92c-84e4-4f64-b7d2-a8b314e35dc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Training our neural network\n",
    "\n",
    "### What is a classifier?\n",
    "\n",
    "Our final goal is to take in a light curve and figure out whether it has a flare or not. We have a large labeled dataset — a list of light curves and the knowledge of whether or not they contain flares — but these examples alone will not be helpful enough to quantitatively classify a new light curve. Our eyes can sort of do the job for us, because flares have a physically motivated shape to them. But, ideally, we want something better than human eyes: something reproducible, something fast, something quantitative, something understandable.\n",
    "\n",
    "Thus enters a set of algorithms known as \"classifiers.\" These programs take in data, and spit out a \"yes/no\" answer. In our case, a classifier would indeed be able to tell us whether or not a lightcurve contains a flare — and perhaps even how confident the algorithm is in its answer.\n",
    "\n",
    "\n",
    "### What is a neural network?\n",
    "\n",
    "Neural networks are a specific type of of program that can be used to solve classification problems. Their namesake is the neuron in a brain, after which neural networks are loosely based. At each layer of a neural network, each neuron receives the weighted output of neurons from the previous layer. It then combines those outputs non-linearly and sends its output to the next layer. This approach allows neural networks to approximate complex and non-linear functions quite well.\n",
    "\n",
    "But approximating functions isn't enough — we need our neural network to figure out what these functions *are*. Doing so involves training, or the act of tuning the weights between neurons mentioned above. Various algorithms (such as <a href=https://www.ibm.com/topics/gradient-descent>gradient descent</a>) have been devised to efficiently and accurately train neural networks.\n",
    "\n",
    "A <a href=https://www.geeksforgeeks.org/introduction-convolution-neural-network/>convolutional neural network</a> is a type of neural network that includes a convolutional layer. This special type of layer performs, as its name implies, a *convolution*. By doing so, the layer can combine information from neighboring features in a dataset. For instance, it can combine information from two neighboring pixels in an image — or, in our case, two neighboring time stamps in a light curve.\n",
    "\n",
    "Please note that this is not an exhaustive explanation of neural networks — and feel free to check out the linked resources at the bottom of this notebook for further information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c4ffe-7038-45d8-bde1-1b7e9e76fd35",
   "metadata": {},
   "source": [
    "Neural nets require some weight initialization. We don't know ahead of time what the weights should be, so they're defined randomly. Setting a *random seed* for our program ensures that while it is random, it is *reproducibly* random. We have to do this a few different ways, because our programs will reference a few different <a href=https://www.freecodecamp.org/news/random-number-generator/>pseudo-random number generators</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb30625-bf74-4e3a-9d84-00e52c189be4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set the numpy random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# set the PYTHONHASHSEED seed\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "\n",
    "# set python built-in random seed\n",
    "random.seed(seed)\n",
    "\n",
    "# set tensorflow random seed\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90891f26-1dd7-4a91-a7f7-4f781ec9f598",
   "metadata": {},
   "source": [
    "We next want to set the filter size of our CNN — that is, how many adjacent pixels it considers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1e1f0-7df1-4eb0-bc7c-dde224c5f46e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filter1 = 16\n",
    "filter2 = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8e0211-dda8-4383-9d70-5947e6f1fc07",
   "metadata": {},
   "source": [
    "Next, we should decide how many neurons to include per layer. Let's try 128 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431aa51e-3119-4a4e-9264-8a499613dce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dense = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c343652-e28b-4edc-a0fb-2fa4dac4fe31",
   "metadata": {
    "tags": []
   },
   "source": [
    "After setting the number of neurons, we set the value for our dropout layers. These layers can help prevent overfitting by killing off some weights to prevent over-complex function representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedab5be-bc19-42d4-8fca-1f7e6307c682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dropout = 0.1 # let's keep this low for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8f58f8-688e-4135-8217-6e71c3380f51",
   "metadata": {},
   "source": [
    "Now, we can set the neural net architecture and define our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a501f-3797-4836-9f53-61218187753e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_shape = np.shape(ds.train_data)\n",
    "input_shape = (np.shape(ds.train_data)[1], 1)\n",
    "\n",
    "# set up the network\n",
    "x_in = Input(shape=input_shape)\n",
    "c0 = Convolution1D(7, filter1, activation='relu', padding='same', input_shape=input_shape)(x_in)\n",
    "b0 = MaxPooling1D(pool_size=2)(c0)\n",
    "d0 = Dropout(dropout)(b0)\n",
    "\n",
    "c1 = Convolution1D(3, filter2, activation='relu', padding='same')(d0)\n",
    "b1 = MaxPooling1D(pool_size=2)(c1)\n",
    "d1 = Dropout(dropout)(b1)\n",
    "\n",
    "\n",
    "f = Flatten()(d1)\n",
    "z0 = Dense(dense, activation='relu')(f)\n",
    "d2 = Dropout(dropout)(z0)\n",
    "y_out = Dense(1, activation='sigmoid')(d2)\n",
    "\n",
    "\n",
    "# instantiate the network\n",
    "cnn = Model(inputs=x_in, outputs=y_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e0115-ce81-4213-af87-a74e6f9e8756",
   "metadata": {},
   "source": [
    "Next, we compile our model. We include arguments that specify how to train the network (the `optimizer`), the quantities to track during training for output (`fit_metrics`), an  how to evlauate whether the model is performing well during training (the `loss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b53d6e-f907-424f-be29-aeb5b1f76874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compile Model\n",
    "optimizer = 'adam'\n",
    "fit_metrics = ['accuracy'] \n",
    "loss = 'binary_crossentropy'\n",
    "cnn.compile(loss=loss, optimizer=optimizer, metrics=fit_metrics)\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e4a1fd-e0e2-462c-b0a4-0b6f0c77248d",
   "metadata": {},
   "source": [
    "That's a lot of parameters! We're starting to get a sense for why neural networks are a powerful technique. With this many parameters, though, we may begin to worry that we're <a href=https://www.ibm.com/topics/overfitting>overfitting</a>. Loosely, \"overfitting\" refers to the idea that our model is too highly tuned to the specific dataset. This means that it's not actually \"learning\" the underlying trend, and it will likely not perform well on a second set of similar data. Thankfully, we can perform some tests as we go to ensure that our model is not overfitting our data.\n",
    "\n",
    "\n",
    "Now, with our model all set up, we can actually train it on the dataset. This process should take roughly a couple of minutes. We'll have to set some details of the training stage now. We begin with the number of epochs. During each epoch, the neural network is trained on all the data. Some terminology:\n",
    "\n",
    "- a *sample* is a single piece of your training set. In our case, it's a light curve.\n",
    "- the *batch size* is how many samples the algorithm looks at before deciding to update its parameters.\n",
    "- the *epoch number* is the number of times that the algorithm works through the whole training set. More epochs means the algorithm \"sees\" the data more times in total — meaning that it has more chances to adjust itself to better predict the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cf6117-185b-4881-99bf-2f5a58436ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100 # the model sees the dataset 100 times\n",
    "batch_size = 64 # 64 light curves are seen before the model updates itself\n",
    "\n",
    "# between each epoch, present the data in a different order. This can help prevent overfitting or order bias.\n",
    "shuffle = True \n",
    "\n",
    "# Train\n",
    "history = cnn.fit(ds.train_data, ds.train_labels,\n",
    "                  batch_size=batch_size, \n",
    "                  epochs=nb_epoch, \n",
    "                  validation_data=(ds.val_data, ds.val_labels), \n",
    "                  shuffle=shuffle,\n",
    "                  verbose=False) # setting verbose=True produces a lot of output. Feel free to change this if you're curious!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb339e8-e618-4087-852c-bec542d57589",
   "metadata": {},
   "source": [
    "Let's save our dataset before proceeding so that we don't have to retrain the whole model if something goes wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68987361-8ac5-4fb9-91e6-697895530dd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the model to file\n",
    "cnn_file = 'initial_flare_model_small_dataset.h5'\n",
    "cnn.save(cnn_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffc8c98-7edf-4256-82e9-b5c91e70ce51",
   "metadata": {},
   "source": [
    "## Training, testing, and validation data\n",
    "One neat thing about *stella* is that it automatically partitions the light curves into categories known as \"training\", \"testing\", and \"validation\" data. The training set contains all the data that your machine learning model will use to actually \"learn\". The the validation set includes data that the model is evaluated against at the end of each epoch, but that doesn't go directly into the training itself. The testing data is a separate set of data that's used at the end of trianing altogether.\n",
    "\n",
    "\n",
    "We can evaluate the success of our training on these categories using a few different metrics. As mentioned above, \"loss\" can loosely be thought of as how well the model performs against the data. At each model iteration, the loss informs the training algorithm's parameter update process. In our case, we set the loss to be \"<a href=https://saturncloud.io/blog/understanding-keras-binary-crossentropy-vs-categorical-crossentropy/>binary_crossentropy</a>.\" This metric is used in classification problems that only have two types of objects; in our case, \"flares\" or \"not flares.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32865d6-3ef4-48a0-9312-f947fed5c2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_loss'], color='tomato', label='Validation loss')\n",
    "plt.plot(history.history['loss'], color='cornflowerblue', label='Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.ylabel('Loss', fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38790027-ad49-4b02-a775-9bbe4ad15d89",
   "metadata": {},
   "source": [
    "In addition to the binary_crossentropy loss metric plotted, we also specified in our midel that we'd like to keep track of \"accuracy.\" In the context of machine learning, accuracy is more simply classified as the fraction of correct predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8105f129-1a97-4454-95fe-24f20e2c536c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(history.history['val_accuracy'], color='tomato',label='Validation accuracy')\n",
    "plt.plot(history.history['accuracy'], color='cornflowerblue', label='Accuracy')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs', fontsize=25)\n",
    "plt.ylabel('Accuracy', fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f0c41-3c4f-4172-b4b8-739a0e999b13",
   "metadata": {},
   "source": [
    "Interesting! Seems like these metrics (both testing and validation accuracy) are noisy and still improving with further epochs. If the model training had gone poorly, our validation accuracy would begin to decrease over time as we overfit the training data. Our plot indicates that we are not yet overfitting — and perhaps that we can even improve the model more with further training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaff6b5-96d7-45de-842d-dcde64724cdd",
   "metadata": {},
   "source": [
    "## The confusion matrix\n",
    "\n",
    "At each training epoch, we're told how the model's performing — its loss, its accuracy, its validation set loss, and its validation set accuracy. These numbers are great to compare against one another (especially when iterating models), but we'd like a global statistic for evaluating our models that's straightforward to understand. That's where the confusion matrix comes in.\n",
    "\n",
    "The confusion matrix tells us: of the flares we're trying to classify, how many of the flares did we correctly identify (true positives; top left), how many did we not identify (false negatives, top right), how many non-flares did we classify as flares (false positives; bottom left), and how many non-flares did we correctly identify (true negatives; bottom right). The rows in a confusion matrix add to 100%. The best-case scenario maximizes the top left and bottom right while minimizing the top right and bottom left. We calculate the confusion matrix with respect to the test data — again, a set of light curves that the model has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47c76f-eb5f-4ee4-a553-a821046fe96f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cnn, input_data, input_labels):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix based on a CNN's predictions.\n",
    "    \n",
    "    Inputs\n",
    "    ------\n",
    "        :cnn: Convolutional neural network, previously trained to classify flares. (keras Functional object)\n",
    "        :input_data: data against which the cnn will be tested.\n",
    "        :input_labels: labels (whether or not input data contains flares) against which the cnn will be tested.\n",
    "        \n",
    "    Outputs\n",
    "    --------\n",
    "        None\n",
    "        \n",
    "    Side effects\n",
    "    ------------\n",
    "        Plots confusion matrix.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute flare predictions for the test dataset\n",
    "    predictions = cnn.predict(input_data)\n",
    "\n",
    "    # Convert to binary classification \n",
    "    predictions = (predictions > 0.5).astype('int32') \n",
    "    \n",
    "    # Compute the confusion matrix by comparing the test labels (ds.test_labels) with the test predictions\n",
    "    cm = metrics.confusion_matrix(input_labels, predictions, labels=[0, 1])\n",
    "    cm = cm.astype('float')\n",
    "\n",
    "    # Normalize the confusion matrix results. \n",
    "    cm_norm = cm / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    # Plotting\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.matshow(cm_norm, cmap='binary_r')\n",
    "\n",
    "    plt.title('Confusion matrix', y=1.08)\n",
    "    \n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_xticklabels(['Flare', 'No Flare'])\n",
    "    \n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_yticklabels(['Flare', 'No Flare'])\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm_norm.max() / 2.\n",
    "    for i in range(cm_norm.shape[0]):\n",
    "        for j in range(cm_norm.shape[1]):\n",
    "            ax.text(j, i, format(cm_norm[i, j], fmt), \n",
    "                    ha=\"center\", va=\"center\",color=\"white\" if cm_norm[i, j] < thresh else \"black\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff10e4a-d43b-4fe0-9b87-be3bf87f1e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cnn, ds.test_data, ds.test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea5c2a-e9d6-4929-ab44-7e724f9df414",
   "metadata": {},
   "source": [
    "This looks pretty good! More than 95% of the true flares were found by our classifier, and less than 20% of the light curves that are not known to exhibit flares were not flagged as flaring. This means that our model is slightly likely to characterize non-flares as true flares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ae8d2a-e4fd-467e-965f-c1aadd3cc294",
   "metadata": {},
   "source": [
    "## Trying the algorithm on real data\n",
    "We have an algothithm that works well enough. Now, let's apply it to real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc85f0-dec9-4358-937f-c24085dcd8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# gather a list of TIC (Tess Input Catalog) identification numbers.\n",
    "\n",
    "ticids = ['120461526', '278779899', '139754153', '273418879', '52121469', '188580272', '394015919', '206544316']\n",
    "\n",
    "\n",
    "# for all the selected targets, pull the available lightcurves using the lightkurve package\n",
    "sectors = [1, 2] # only consider the first few sectors\n",
    "lcs = []\n",
    "for name in ticids:\n",
    "    lc = search_lightcurve(target='TIC'+name, mission='TESS', sector=sectors, author='SPOC')\n",
    "    lc = lc.download_all()\n",
    "    lcs.append(lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ad919-bfc1-4eca-a0ad-46db82ecddd5",
   "metadata": {},
   "source": [
    "Now that we're using \"real\", untested data, we should do a quick check to make sure that all of the data is finite. If not, that could cause some issues with our flare detection!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de76c30-ff74-4090-a6ee-be1c0f96dd14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quality checks: see if there are non-finite numbers in the light curve\n",
    "for lc in lcs:\n",
    "    if len(lc) > 0: \n",
    "        lc = lc[0] \n",
    "    if not np.all(np.isfinite(lc.flux)):\n",
    "        print('Non-finite number found!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cdae1d-2e22-4288-997a-19fd027b78af",
   "metadata": {},
   "source": [
    "We saved our trained model from above, so let's load that in now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119c631-4fff-4a27-a8e9-f1cf64be2a69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the CNN using `stella`\n",
    "cnn_stella = stella.ConvNN(output_dir=data_dir, ds=ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7cb577-c993-4d0f-a2fa-2bab358c381a",
   "metadata": {},
   "source": [
    "We're going to do two things at once, as we loop through the light curves:\n",
    "1. Use stella to predict the liklihood of a flare for each light curve.\n",
    "2. Create a plot for each light curve, where the likelihood of a flare is represented by the color of the data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7084f1e7-de92-4fd8-97f4-7467c92e1c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the overall figure\n",
    "fig = plt.figure(0, [8, 10])\n",
    "\n",
    "# Loop the light curves\n",
    "for i, lc in enumerate(lcs):\n",
    "    \n",
    "    # pull out on the first light curve in each set, if more than one exist\n",
    "    if len(lc)>0: lc = lc[0]\n",
    "        \n",
    "    # predict the flare probability for the input light curve using `stella` \n",
    "    cnn_stella.predict(cnn_file, times=lc.time.value, fluxes=lc.flux, errs=lc.flux_err)\n",
    "    \n",
    "    # add a plot for the lightcurve we just analyzed\n",
    "    ax = fig.add_subplot(4,2,i+1)\n",
    "    im = ax.scatter(cnn_stella.predict_time[0], cnn_stella.predict_flux[0], c=cnn_stella.predictions[0], s=1. )\n",
    "    \n",
    "    # Format our plots nicely\n",
    "    plt.colorbar(im, ax=ax, label='Probability of Flare')\n",
    "    ax.set_xlabel('Time [BJD-2457000]')\n",
    "    ax.set_ylabel('Normalized Flux')\n",
    "    ax.set_title('TIC {}'.format(lc.targetid));\n",
    "\n",
    "# Plot the overall figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3b5ac4-7dac-48db-bd11-0ae60b4fff9b",
   "metadata": {},
   "source": [
    "There's relatively low flare probability in most of these lightcurves, but that last one has some very obvious flares!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f4a57-3284-4bf2-a4f2-6e94018db2f7",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "- You may have arrived at a false positive rate (bottom left corner of the confusion matrix) of roughly 30%. Improve this rate first using more complex model architecture (more layers, denser layers)\n",
    "- Now do the same, but by using more data. Which approach gives you a \"better\" confusion matrix? Which approach is more prone to overfitting? How does the class imbalance affect both overfitting and the confusion matrix?\n",
    "- You may have noticed that this algorithm picks out regions near data gaps as having high probabilities of having flares, even if by eye they seem to not be flaring. This is more of a bug than a feature, and addressing it would likely be difficult to do in the context of a CNN. Rather than hack the model itself, we can refine our final predictions by masking those that make sense. That is, to arrive at cleaner results, we can simply reject results that lie too close to data gaps. Do so, and see how your results change.\n",
    "- Given what we know about flares, why might CNNs be a good architecture choice for detecting / classifying them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6fc4a-2c99-405d-b8a5-eb71e3c05daf",
   "metadata": {},
   "source": [
    "# Resources\n",
    "- <a href=https://www.ibm.com/topics/overfitting> IBM cloud on overfitting </a>\n",
    "- <a href=https://www.cambridge.org/core/journals/international-astronomical-union-colloquium/article/review-of-stellar-flares-and-their-characteristics/F23B5B96E8450CD685D00988C0579B8C> Stellar flares review </a>\n",
    "- <a href=https://github.com/afeinstein20/stella> stella GitHub repository </a>\n",
    "- <a href=https://adina.feinste.in/stella/> stella documentation </a>\n",
    "- <a href=https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414> high-level article on neural networks </a>\n",
    "- <a href=https://www.quantamagazine.org/how-artificial-intelligence-is-changing-science-20190311/> Quanta article on AI in science </a>\n",
    "- <a href=https://medium.com/analytics-vidhya/what-is-a-confusion-matrix-d1c0f8feda5> article on confusion matrices</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TESS Environment",
   "language": "python",
   "name": "tess"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
